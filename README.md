# AnyCompany Food & Beverage â€“ Data-Driven Marketing Analytics (Snowflake + Streamlit + ML)

Projet rÃ©alisÃ© dans le cadre du workshop **Data-Driven Marketing Analytics avec Snowflake et Streamlit**.  
Objectif : construire un socle analytique fiable (ingestion + nettoyage), produire des analyses business, puis industrialiser ces analyses sous forme de **data product** prÃªt pour la BI et le Machine Learning.

## ğŸ” AccÃ¨s Snowflake


- **URL** : https://pcvplxy-rrb95749.snowflakecomputing.com  
- **Login** : mbaesg  
- **Password** : Test@123456@123  
- **Database** : ANYCOMPANY_LAB  
- **Warehouse** : WH_LAB  



## 1) Contexte & objectif business

AnyCompany (entreprise fictive) subit :
- une baisse de ventes sur le dernier exercice fiscal,
- une rÃ©duction de 30% du budget marketing,
- une perte de part de marchÃ© (28% â†’ 22% en 8 mois).

**Objectif** : rÃ©orienter le marketing vers une approche data-driven afin de :
- inverser la tendance,
- viser **+10 points de part de marchÃ©** (22% â†’ 32%) dâ€™ici T4 2025,
- optimiser les actions avec un budget rÃ©duit.

---

## 2) Architecture et approche

Nous avons construit une architecture en 3 couches :

- **BRONZE** : donnÃ©es brutes (raw) issues des fichiers CSV/JSON
- **SILVER** : donnÃ©es nettoyÃ©es, cohÃ©rentes et exploitables
- **ANALYTICS** : data product (tables stables avec KPIs et flags utiles)

Source des donnÃ©es : `s3://logbrain-datalake/datasets/food-beverage/`

---

## 3) Phase 1 â€“ Data Preparation & Ingestion (Snowflake)

### 3.1 Ã‰tape 1 â€” PrÃ©paration de lâ€™environnement Snowflake

#### 3.1.1 CrÃ©ation du warehouse
Un warehouse `XSMALL` a Ã©tÃ© utilisÃ© pour limiter la consommation de crÃ©dits, avec auto-suspend activÃ©.

```sql
CREATE OR REPLACE WAREHOUSE WH_LAB
WITH
  WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 60
  AUTO_RESUME = TRUE;

USE WAREHOUSE WH_LAB;
```

#### 3.1.2 CrÃ©ation de la base et des schÃ©mas
Nous avons crÃ©Ã© une base dÃ©diÃ©e au lab, puis 3 schÃ©mas :
- `BRONZE` (raw)
- `SILVER` (clean)
- `ANALYTICS` (data product)

```sql
CREATE OR REPLACE DATABASE ANYCOMPANY_LAB;

CREATE OR REPLACE SCHEMA ANYCOMPANY_LAB.BRONZE;
CREATE OR REPLACE SCHEMA ANYCOMPANY_LAB.SILVER;
CREATE OR REPLACE SCHEMA ANYCOMPANY_LAB.ANALYTICS;
```

---

### 3.2 Ã‰tape 2 â€” File formats & Stage S3

#### 3.2.1 Formats de fichiers

**CSV standard** (dÃ©limiteur virgule) :

```sql
CREATE OR REPLACE FILE FORMAT FF_CSV
  TYPE = CSV
  FIELD_DELIMITER = ','
  SKIP_HEADER = 1
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  TRIM_SPACE = TRUE
  NULL_IF = ('', 'NULL');
```

**TSV (tabulation)** : utilisÃ© pour `product_reviews.csv` car le fichier Ã©tait en rÃ©alitÃ© sÃ©parÃ© par tabulations.  
Sans cela, on obtenait lâ€™erreur : *"Number of columns in file does not match table"*.  
Nous avons ajoutÃ© `ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE` pour Ã©viter lâ€™Ã©chec complet du chargement.

```sql
CREATE OR REPLACE FILE FORMAT FF_TSV
  TYPE = CSV
  FIELD_DELIMITER = '\t'
  SKIP_HEADER = 1
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  TRIM_SPACE = TRUE
  NULL_IF = ('', 'NULL', 'null')
  ERROR_ON_COLUMN_COUNT_MISMATCH = FALSE;
```

**JSON** : les JSON fournis Ã©taient sous forme de tableau â†’ `STRIP_OUTER_ARRAY = TRUE`.

```sql
CREATE OR REPLACE FILE FORMAT FF_JSON
  TYPE = JSON
  STRIP_OUTER_ARRAY = TRUE;
```

#### 3.2.2 Stage S3
```sql
CREATE OR REPLACE STAGE STG_FOOD_BEVERAGE
  URL = 's3://logbrain-datalake/datasets/food-beverage/'
  FILE_FORMAT = FF_CSV;

LIST @STG_FOOD_BEVERAGE;
```

---

### 3.3 Ã‰tape 2 â€” CrÃ©ation des tables BRONZE (raw)

Une table BRONZE a Ã©tÃ© crÃ©Ã©e pour chaque fichier.  
Pour les JSON, nous avons stockÃ© les lignes dans une colonne `VARIANT` (`raw`).

Exemples :

- **CSV** : types adaptÃ©s aux analyses (DATE, NUMBER(18,2), etc.)
- **JSON** : table `raw VARIANT`

```sql
CREATE OR REPLACE TABLE BRONZE.INVENTORY_RAW ( raw VARIANT );
CREATE OR REPLACE TABLE BRONZE.STORE_LOCATIONS_RAW ( raw VARIANT );
```

---

### 3.4 Ã‰tape 3 â€” Chargement des donnÃ©es (COPY INTO)

Pour chaque table BRONZE, nous avons chargÃ© les donnÃ©es depuis le stage S3 avec `COPY INTO`, en utilisant le `FILE_FORMAT` adaptÃ©.

Exemples :

```sql
COPY INTO BRONZE.CUSTOMER_DEMOGRAPHICS
FROM @STG_FOOD_BEVERAGE/customer_demographics.csv
FILE_FORMAT = (FORMAT_NAME = FF_CSV)
ON_ERROR = 'CONTINUE';
```

`product_reviews.csv` (TSV) :

```sql
COPY INTO BRONZE.PRODUCT_REVIEWS
FROM @STG_FOOD_BEVERAGE/product_reviews.csv
FILE_FORMAT = (FORMAT_NAME = FF_TSV)
ON_ERROR = 'CONTINUE';
```

JSON :

```sql
COPY INTO BRONZE.INVENTORY_RAW
FROM @STG_FOOD_BEVERAGE/inventory.json
FILE_FORMAT = (FORMAT_NAME = FF_JSON)
ON_ERROR = 'CONTINUE';
```

#### VÃ©rification COPY (historique)
```sql
SELECT *
FROM TABLE(
  INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME=>'FINANCIAL_TRANSACTIONS',
    START_TIME=>DATEADD('hour', -2, CURRENT_TIMESTAMP())
  )
);
```

---

### 3.5 Ã‰tape 4 â€” VÃ©rifications post-chargement (BRONZE)

AprÃ¨s chaque chargement, nous avons systÃ©matiquement :
- vÃ©rifiÃ© les **volumes** (table vide / chargement partiel),
- inspectÃ© un **Ã©chantillon** (`LIMIT 10`),
- identifiÃ© les **colonnes clÃ©s** (IDs, dates, rÃ©gions, catÃ©gories),
- dÃ©tectÃ© les anomalies Ã©videntes (valeurs nÃ©gatives, dates invalides, etc.).

Exemple de contrÃ´le volume global :

```sql
SELECT 'CUSTOMER_DEMOGRAPHICS' AS table_name, COUNT(*) AS nb_rows FROM BRONZE.CUSTOMER_DEMOGRAPHICS
UNION ALL SELECT 'FINANCIAL_TRANSACTIONS', COUNT(*) FROM BRONZE.FINANCIAL_TRANSACTIONS
UNION ALL SELECT 'STORE_LOCATIONS_RAW', COUNT(*) FROM BRONZE.STORE_LOCATIONS_RAW
ORDER BY table_name;
```

---

## 4) Phase 1 â€“ Ã‰tape 5 : Data Cleaning (BRONZE â†’ SILVER)

### 4.1 Principes de nettoyage appliquÃ©s

Pour chaque table BRONZE, nous avons crÃ©Ã© une table SILVER en appliquant :

1. **Nettoyage des champs texte**
   - `TRIM()`, `NULLIF(TRIM(x), '')` pour Ã©viter les chaÃ®nes vides
2. **Harmonisation des types**
   - Dates : `TRY_TO_DATE(...)`
   - NumÃ©riques : `TRY_TO_DECIMAL(...)`, suppression des espaces (`REPLACE(x,' ','')`)
3. **RÃ¨gles de qualitÃ©**
   - montants positifs (transactions)
   - discount entre 0 et 1 (promotions)
   - rating entre 1 et 5 (reviews)
   - coÃ»ts de shipping >= 0
   - lead_time, stock, reorder_point >= 0
4. **Gestion des doublons (IDs)**
   - RÃ¨gle gÃ©nÃ©rale : **si un ID est censÃ© Ãªtre unique, on dÃ©doublonne**
   - MÃ©thode : `QUALIFY ROW_NUMBER()` avec un critÃ¨re mÃ©tier (date la plus rÃ©cente ou â€œligne la plus complÃ¨teâ€)
5. **Suppression des lignes avec la region 0 et 1 dans la table PROMOTION**

---

### 4.2 Exemple : Financial Transactions

**Objectif** : sÃ©curiser la base de ventes (montants exploitables, IDs uniques, dates valides).

- DÃ©doublonnage sur `transaction_id`
- Conversion robuste du montant
- Suppression des montants nuls ou nÃ©gatifs

```sql
CREATE OR REPLACE TABLE SILVER.FINANCIAL_TRANSACTIONS_CLEAN AS
SELECT
  TRIM(transaction_id) AS transaction_id,
  TRY_TO_DATE(transaction_date::VARCHAR) AS transaction_date,
  TRIM(transaction_type) AS transaction_type,
  TRY_TO_DECIMAL(REPLACE(amount::VARCHAR, ' ', ''), 18, 2) AS amount,
  TRIM(payment_method) AS payment_method,
  TRIM(entity) AS entity,
  NULLIF(TRIM(region), '') AS region,
  TRIM(account_code) AS account_code
FROM BRONZE.FINANCIAL_TRANSACTIONS
WHERE transaction_id IS NOT NULL AND TRIM(transaction_id) <> ''
QUALIFY ROW_NUMBER() OVER (
  PARTITION BY TRIM(transaction_id)
  ORDER BY TRY_TO_DATE(transaction_date::VARCHAR) DESC NULLS LAST
) = 1;

DELETE FROM SILVER.FINANCIAL_TRANSACTIONS_CLEAN
WHERE amount IS NULL OR amount <= 0;
```

---

### 4.3 Exemple : Promotions

- validation de la pÃ©riode (start_date <= end_date)
- discount entre 0 et 1
- Suppression des lignes avec region 0 et 1.

```sql
CREATE OR REPLACE TABLE SILVER.PROMOTIONS_CLEAN AS
SELECT
  TRIM(promotion_id) AS promotion_id,
  NULLIF(TRIM(product_category), '') AS product_category,
  NULLIF(TRIM(promotion_type), '') AS promotion_type,
  TRY_TO_DOUBLE(discount_percentage) AS discount_percentage,
  TRY_TO_DATE(start_date::VARCHAR) AS start_date,
  TRY_TO_DATE(end_date::VARCHAR) AS end_date,
  NULLIF(TRIM(region), '') AS region
FROM BRONZE.PROMOTIONS_DATA
WHERE promotion_id IS NOT NULL
  AND TRIM(promotion_id) <> ''
  AND TRY_TO_DATE(start_date::VARCHAR) IS NOT NULL
  AND TRY_TO_DATE(end_date::VARCHAR) IS NOT NULL
  AND TRY_TO_DATE(start_date::VARCHAR) <= TRY_TO_DATE(end_date::VARCHAR)
  AND TRY_TO_DOUBLE(discount_percentage) BETWEEN 0 AND 1
QUALIFY ROW_NUMBER() OVER (
  PARTITION BY TRIM(promotion_id)
  ORDER BY TRY_TO_DATE(start_date::VARCHAR) DESC NULLS LAST
) = 1;
```

---

### 4.4 Cas particulier important : STORE_LOCATIONS (IDs dupliquÃ©s)

#### Constat
Dans la table `BRONZE.STORE_LOCATIONS_RAW`, nous avions environ **5000 lignes**.  
AprÃ¨s nettoyage et dÃ©doublonnage par `store_id`, il ne restait que **897 lignes**, donc **~82% des donnÃ©es supprimÃ©es**.

Cela signifie que :
- les `store_id` Ã©taient fortement dupliquÃ©s,
- MAIS les lignes associÃ©es Ã  un mÃªme `store_id` avaient **des valeurs diffÃ©rentes** (donc ce ne sont pas de vrais doublons).

#### Alternatives envisagÃ©es
Nous avons envisagÃ© des approches plus avancÃ©es :

1) **Changer lâ€™identifiant**
- crÃ©er un identifiant technique (surrogate key) : `store_id + hash(address + city + country)`
- permet de conserver toutes les lignes

2) **CrÃ©er des versions (SCD Type 2)**
- conserver lâ€™historique des changements avec `valid_from / valid_to`
- nÃ©cessite un champ date fiable (par exemple `updated_at`)

#### Pourquoi nous ne les avons pas retenues
Les donnÃ©es du workshop sont **gÃ©nÃ©rÃ©es** et ne contiennent **pas de champ date** (ex : updated_at) permettant de gÃ©rer les versions correctement.  
Sans date, il est impossible de savoir :
- quelle ligne est la version â€œcouranteâ€
- quelle ligne est la version â€œancienneâ€

#### DÃ©cision finale
Nous avons choisi une approche simple et cohÃ©rente avec lâ€™objectif pÃ©dagogique :

âœ… **DÃ©doublonner sur `store_id` en gardant la ligne la plus complÃ¨te**  
(mÃ©thode `completeness_score` + `ROW_NUMBER()`)

```sql
CREATE OR REPLACE TABLE SILVER.STORE_LOCATIONS_CLEAN AS
WITH parsed AS (
  SELECT
    NULLIF(TRIM(raw:store_id::STRING), '') AS store_id,
    NULLIF(TRIM(raw:store_name::STRING), '') AS store_name,
    NULLIF(TRIM(raw:store_type::STRING), '') AS store_type,
    NULLIF(TRIM(raw:region::STRING), '') AS region,
    NULLIF(TRIM(raw:country::STRING), '') AS country,
    NULLIF(TRIM(raw:city::STRING), '') AS city,
    NULLIF(TRIM(raw:address::STRING), '') AS address,
    NULLIF(TRIM(raw:postal_code::STRING), '') AS postal_code,
    IFF(TRY_TO_DOUBLE(raw:square_footage::STRING) > 0,
        TRY_TO_DOUBLE(raw:square_footage::STRING),
        NULL) AS square_footage,
    IFF(TRY_TO_NUMBER(raw:employee_count::STRING) >= 0,
        TRY_TO_NUMBER(raw:employee_count::STRING),
        NULL) AS employee_count,
    (
      IFF(NULLIF(TRIM(raw:store_name::STRING), '') IS NULL, 0, 1) +
      IFF(NULLIF(TRIM(raw:country::STRING), '') IS NULL, 0, 1) +
      IFF(NULLIF(TRIM(raw:city::STRING), '') IS NULL, 0, 1) +
      IFF(TRY_TO_DOUBLE(raw:square_footage::STRING) > 0, 1, 0) +
      IFF(TRY_TO_NUMBER(raw:employee_count::STRING) >= 0, 1, 0)
    ) AS completeness_score
  FROM BRONZE.STORE_LOCATIONS_RAW
)
SELECT
  store_id, store_name, store_type, region, country, city, address, postal_code,
  square_footage, employee_count
FROM parsed
WHERE store_id IS NOT NULL
QUALIFY ROW_NUMBER() OVER (
  PARTITION BY store_id
  ORDER BY completeness_score DESC
) = 1;
```

---

## 5) Phase 2 â€“ Analyses exploratoires & business (SILVER)

Les analyses ont Ã©tÃ© rÃ©alisÃ©es Ã  partir des tables SILVER, en couvrant :
- Ã©volution des ventes dans le temps,
- performance par rÃ©gion,
- impact promotions (par rÃ©gion + pÃ©riode),
- ROI proxy des campagnes,
- ratings par catÃ©gorie,
- SAV (satisfaction),
- ruptures de stock,
- dÃ©lais de livraison.

Les scripts SQL sont regroupÃ©s dans `sql/` (1 fichier par analyse).

---

## 6) Phase 3 â€“ Data Product (ANALYTICS)

### Objectif  
Industrialiser les insights issus de la Phase 2 en **tables analytiques rÃ©utilisables**, stables et prÃªtes Ã  Ãªtre consommÃ©es par :
- des dashboards (Streamlit / BI),
- des analyses avancÃ©es,
- des modÃ¨les de Machine Learning.

Cette phase correspond Ã  un travail dâ€™**Analytics Engineering** : on transforme des analyses ponctuelles en **produits data durables**.

---

### Tables crÃ©Ã©es dans le schÃ©ma `ANALYTICS`

#### `ANALYTICS.SALES_ENRICHED`
**Objectif mÃ©tier**  
Centraliser les ventes et les enrichir avec des indicateurs marketing afin de mesurer lâ€™impact rÃ©el :
- des promotions,
- des campagnes marketing,
- du facteur temps.

**Contenu**
- DonnÃ©es de vente (transaction, date, rÃ©gion, montant)
- Flags analytiques :
  - pÃ©riode de promotion
  - pÃ©riode de campagne
- Variables temporelles (mois, jour de la semaine)

**Cas dâ€™usage**
- Analyse ROI marketing
- Comparaison ventes avec / sans promotion
- Base pour modÃ¨les de prÃ©vision des ventes

**Tables sources utilisÃ©es**
- `SILVER.FINANCIAL_TRANSACTIONS_CLEAN`
- `SILVER.PROMOTIONS_CLEAN`
- `SILVER.MARKETING_CAMPAIGNS_CLEAN`

---

#### `ANALYTICS.ACTIVE_PROMOTIONS`
**Objectif mÃ©tier**  
Disposer dâ€™une table normalisÃ©e des promotions pour analyser leur efficacitÃ© selon :
- la catÃ©gorie produit,
- la rÃ©gion,
- la durÃ©e.

**Contenu**
- Promotion, catÃ©gorie, rÃ©gion
- Discount appliquÃ©
- Dates de dÃ©but et de fin
- DurÃ©e de la promotion (en jours)

**Cas dâ€™usage**
- Analyse de la sensibilitÃ© aux promotions
- Optimisation du calendrier promotionnel

**Tables sources utilisÃ©es**
- `SILVER.PROMOTIONS_CLEAN`

---

#### `ANALYTICS.CUSTOMERS_ENRICHED`
**Objectif mÃ©tier**  
CrÃ©er une table client enrichie pour permettre une **segmentation marketing avancÃ©e**.

**Contenu**
- Informations dÃ©mographiques
- Ã‚ge calculÃ©
- Segment de revenu (Low / Medium / High)

**Cas dâ€™usage**
- Ciblage marketing
- Scoring client
- Base pour modÃ¨les de churn ou de valeur client

**Tables sources utilisÃ©es**
- `SILVER.CUSTOMER_DEMOGRAPHICS_CLEAN`

---

### Exemple : crÃ©ation de `ANALYTICS.SALES_ENRICHED`

```sql
CREATE OR REPLACE TABLE ANALYTICS.SALES_ENRICHED AS
WITH sales AS (
  SELECT transaction_id, transaction_date, region, amount
  FROM SILVER.FINANCIAL_TRANSACTIONS_CLEAN
  WHERE transaction_type = 'Sale'
),
promo_flag AS (
  SELECT s.transaction_id, MAX(1) AS is_promo_period
  FROM sales s
  JOIN SILVER.PROMOTIONS_CLEAN p
    ON s.region = p.region
   AND s.transaction_date BETWEEN p.start_date AND p.end_date
  GROUP BY s.transaction_id
),
campaign_flag AS (
  SELECT s.transaction_id, MAX(1) AS is_campaign_period
  FROM sales s
  JOIN SILVER.MARKETING_CAMPAIGNS_CLEAN c
    ON s.region = c.region
   AND s.transaction_date BETWEEN c.start_date AND c.end_date
  GROUP BY s.transaction_id
)
SELECT
  s.transaction_id,
  s.transaction_date,
  s.region,
  s.amount,
  COALESCE(p.is_promo_period, 0) AS is_promo_period,
  COALESCE(c.is_campaign_period, 0) AS is_campaign_period,
  DATE_TRUNC('month', s.transaction_date) AS sales_month,
  DAYOFWEEK(s.transaction_date) AS day_of_week
FROM sales s
LEFT JOIN promo_flag p ON s.transaction_id = p.transaction_id
LEFT JOIN campaign_flag c ON s.transaction_id = c.transaction_id;
```

### RÃ©sultat de la Phase 3

Ã€ lâ€™issue de cette phase, le projet dispose :

- dâ€™un **Data Product analytique cohÃ©rent**, construit Ã  partir de donnÃ©es nettoyÃ©es et validÃ©es ;
- de **tables analytiques documentÃ©es et rÃ©utilisables**, centralisÃ©es dans le schÃ©ma `ANALYTICS` ;
- dâ€™un **socle data prÃªt Ã  lâ€™emploi** pour :
  - la crÃ©ation de dashboards dÃ©cisionnels avec **Streamlit**,
  - des **analyses marketing avancÃ©es** (ROI, segmentation, performance des campagnes),
  - le dÃ©veloppement de **modÃ¨les de Machine Learning** orientÃ©s marketing (segmentation clients, propension Ã  lâ€™achat, rÃ©ponse aux promotions).


---

## 7) Streamlit (dashboards)

Une page par analyse (multi-pages Streamlit) :
- Sales Dashboard
- Promotion Analysis
- Marketing ROI
- Customer Segmentation
- Operations & Logistics

Connexion Snowflake via `.streamlit/secrets.toml` (non versionnÃ©).

---
## 8) Structure du projet

```text
SNOWFLAKE/
â”œâ”€â”€ ml/
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ phase_1/
â”‚ â”‚ â”œâ”€â”€ 1_prÃ©paration_environnement.sql
â”‚ â”‚ â”œâ”€â”€ 2_creation_tables.sql
â”‚ â”‚ â”œâ”€â”€ 3_chargement_donnÃ©e.sql
â”‚ â”‚ â”œâ”€â”€ 4_verification_chargement.sql
â”‚ â”‚ â””â”€â”€ 5_clean_data.sql
â”‚ â”œâ”€â”€ phase_2/
â”‚ â”‚ â”œâ”€â”€ 1_comprehension_donnÃ©.sql
â”‚ â”‚ â”œâ”€â”€ 2_analyses_exploratoires_descriptives.sql
â”‚ â”‚ â”œâ”€â”€ 3.1_promotion_impact.sql
â”‚ â”‚ â”œâ”€â”€ 3.2_campaign_performance.sql
â”‚ â”‚ â”œâ”€â”€ 3.3_experience_client.sql
â”‚ â”‚ â””â”€â”€ 3.4_operation_et_logistique.sql
â”‚ â””â”€â”€ phase_3/
â”‚ â”œâ”€â”€ 1_creation_data_product.sql
â”‚ â””â”€â”€ 2_ml_feature_tables.sql
â”œâ”€â”€ streamlit/
â”‚ â”œâ”€â”€ .streamlit/
â”‚ â”‚ â”œâ”€â”€ config.toml
â”‚ â”‚ â””â”€â”€ secrets.toml
â”‚ â”œâ”€â”€ ml_models/
â”‚ â”œâ”€â”€ pages/
â”‚ â”œâ”€â”€ _utils.py
â”‚ â”œâ”€â”€ check_databases.py
â”‚ â”œâ”€â”€ check_sql_ready.py
â”‚ â””â”€â”€ Home.py
â”œâ”€â”€ business_insights.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```


---
## 9) AI-Powered Promo Planning** - Predict promotion ROI before launch with ML models!

---

## ğŸš€ Quick Start - Run the ML Demo

```powershell
# 1. Create ML feature tables in Snowflake
# Execute: sql/phase_3/2_ml_feature_tables.sql

# 2. Train ML models (2 minutes)
python streamlit/ml_models/promo_optimizer.py

# 3. Launch Streamlit
cd streamlit
python -m streamlit run Home.py

# 4. Navigate to "7_Promo_Planner" and predict ROI!
```

ğŸ“– **Full ML Guide**: See `ML_IMPLEMENTATION_README.md`

---


## 10) Notes et limites

- Les donnÃ©es Ã©tant **gÃ©nÃ©rÃ©es**, certaines clÃ©s (ex : `store_id`) ne reflÃ¨tent pas toujours une logique mÃ©tier rÃ©aliste.
- Absence de `customer_id` dans les transactions : les analyses ventes/clients restent sÃ©parÃ©es.
- Absence de `product_id` dans les ventes : analyses produit effectuÃ©es par proxy (catÃ©gorie/rÃ©gion/pÃ©riode).

---

## 11) Lancer lâ€™app Streamlit

Depuis la dossier streamlit :

```bash
streamlit run Home.py
```

---
